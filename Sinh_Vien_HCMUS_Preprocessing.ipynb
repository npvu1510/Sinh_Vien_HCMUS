{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Giới thiệu**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thành viên:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhóm `Sinh viên HCMUS`:\n",
    "- Nguyễn Khắc Vỹ - 19127637\n",
    "- Nguyễn Phan Vũ - 19127633"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset:` Cơ sở dữ liệu lớn về sách. Sách thuộc nhiều thể loại khác nhau, từ hàng ngàn tác giả khác nhau. Không chỉ tiếng Anh mà còn bao gồm tiếng Ấn Độ, cùng với đó là nhiều thông tin chi tiết khác về từng cuốn sách.\n",
    "\n",
    "`Competition:` **Xây dựng mô hình dự đoán giá của sách** trên **MachineHack**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Title:`** Tên sách\n",
    "\n",
    "**`Author:`** Tác giả\n",
    "\n",
    "**`Edition:`** Phiên bản phát hành\n",
    "\n",
    "**`Reviews:`** Điểm trung bình\n",
    "\n",
    "**`Ratings:`** Số lượt chấm điểm\n",
    "\n",
    "**`Synopsis:`** Tóm tắt\n",
    "\n",
    "**`Genre:`** Thể loại\n",
    "\n",
    "**`BookCategory:`** Category của sách\n",
    "\n",
    "**`Price:`** Giá sách"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scale:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Training set:`** 6237 records\n",
    "\n",
    "**`Test set:`** 1560 records"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Coding**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libiraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string \n",
    "import os\n",
    "\n",
    "# Preprocessing \n",
    "from googletrans import Translator, constants\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "# NLTK preprocessing\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df=pd.read_excel('Data_Train.xlsx')\n",
    "test_data_df=pd.read_excel('Data_Test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tập train: \", train_data_df.shape)\n",
    "print(\"Tập test: \", test_data_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`Concat`** 2 tập **`train`** và **`test`** để preprocessing đồng thời"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['Set'] = 'train'\n",
    "test_data_df['Set'] = 'test'\n",
    "\n",
    "data_df = pd.concat([train_data_df, test_data_df])\n",
    "data_df = data_df.reset_index(drop=True)\n",
    "data_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dữ liệu sau khi concat: \", data_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xử lý từng **`vấn đề`**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cột **`Author`**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiện tại, **`Author`** chứa tên tác giả, một cuốn sách có thể được đồng sáng tác bởi nhiều tác giả.\n",
    "\n",
    "Tuy nhiên, chứa các kí tự ngăn cách giữa các tác giả không đồng nhất => Khiến cho có thể sách do cùng một nhóm tác giả sáng tác, khi count lại được tính là 2 nhóm khác nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_punctuations(texts):\n",
    "  set_of_punctuations = set()\n",
    "  for text in texts:\n",
    "    for punc in string.punctuation:\n",
    "      if punc in text:\n",
    "        set_of_punctuations.add(punc)\n",
    "  return set_of_punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = data_df['Author'].copy()\n",
    "\n",
    "find_unique_punctuations(authors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace các dấu này, về **`','`** cho đồng nhất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa một số loại dấu tương ứng với các hành động riêng biệt\n",
    "\n",
    "# Kí tự không mang quá nhiều ý nghĩa\n",
    "punctuations_to_remove = ['(',')','!',\"'\",'.']\n",
    "\n",
    "# Kí tự '-' thường mang ý nghĩa tương tự dấu cách => thay bằng spacebar\n",
    "punctuations_to_replace_with_space = ['-']\n",
    "\n",
    "# Kí tự mang thường ý nghĩa 'and' đồng nhất bằng dấu ','\n",
    "punctuations_to_replace_with_comma = ['&', '/', ';']\n",
    "\n",
    "def handle_author_text(text):\n",
    "   # Makes the text to lowercase\n",
    "   text = text.lower()\n",
    "\n",
    "   # Handling each punctuation case \n",
    "   for punctuation in punctuations_to_remove:\n",
    "     text = text.replace(punctuation, '')\n",
    "   for punctuation in punctuations_to_replace_with_comma:\n",
    "     text = text.replace(punctuation, ', ')\n",
    "   for punctuation in punctuations_to_replace_with_space:\n",
    "     text = text.replace(punctuation, ' ')\n",
    "\n",
    "   return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique trước khi preprocess:\", data_df['Author'].nunique())\n",
    "\n",
    "authors = authors.apply(handle_author_text)\n",
    "authors.nunique()\n",
    "print(\"Unique trước khi preprocess:\", authors.nunique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Thêm cột **`số lượng tác giả`** cho sách"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make integers based of different authors\n",
    "def count_authors(text):\n",
    "  return text.count(',')+1\n",
    "\n",
    "num_of_authors = authors.apply(count_authors)\n",
    "#data = num_of_authors.to_frame()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đã đồng nhất các giá trị trong cột => Vì số giá trị unique giảm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cột **`Ratings`**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiện tại, ratings không chỉ bao gồm số (số lượt rating), mà còn bao gồm text (không cần thiết)\n",
    "\n",
    "**`Ví dụ:`** 13 customer reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = data_df['Ratings']\n",
    "ratings[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.apply(lambda x: (x.split()[0].replace(',',''))).astype(int)\n",
    "ratings[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cột **`Reviews`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = data_df['Reviews']\n",
    "reviews[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.apply(lambda x: x.split(' ')[0])\n",
    "reviews = reviews.astype(np.float16)\n",
    "reviews[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cột **`Title`** và **`Synopsis`**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phần **`Giới thiệu`** có đề cập dataset có những quyển được gather từ nguồn của Ấn Độ vì vậy **`Synopsis`** không chỉ có mỗi Tiếng Anh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Vấn đề đồng nhất **`ngôn ngữ`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_synopses = data_df['Title'] + \" \" + data_df['Synopsis']\n",
    "titles_synopses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('titles_synopses_df.csv'):\n",
    "  translator = Translator()\n",
    "  translator.raise_Exception = True\n",
    "\n",
    "  # Initilize the Google API translator\n",
    "  new_titles_synopses = []\n",
    "  for i, synopsis in enumerate(tqdm(titles_synopses)):\n",
    "      # Using a sleep timer in order not to get timetout from google's API\n",
    "      time.sleep(0.25)\n",
    "      # Detect the language\n",
    "      try:\n",
    "        detection = translator.detect(synopsis)\n",
    "        # If language is english with high confidence then don't translate\n",
    "        if not ((detection.lang == \"en\")):\n",
    "          translation = translator.translate(synopsis, dest=\"en\")\n",
    "          print(f\"{translation.origin} ({translation.src}) --> {translation.text} ({translation.dest})\")\n",
    "          new_titles_synopses.append(translation.text)\n",
    "        else:\n",
    "          new_titles_synopses.append(synopsis)\n",
    "      except Exception as e:\n",
    "          # print(e, \"for document\", i)\n",
    "          new_titles_synopses.append(synopsis)\n",
    "          \n",
    "  # Calling DataFrame constructor on list\n",
    "  new_titles_synopses_df = pd.DataFrame(new_titles_synopses)\n",
    "  new_titles_synopses_df.to_csv('titles_synopses_df.csv', index=False)\n",
    "\n",
    "else:\n",
    "  # Else load the translated text from the .csv file\n",
    "  translated_titles_synopses_df = pd.read_csv('titles_synopses_df.csv')\n",
    "  translated_titles_synopses_df = translated_titles_synopses_df.rename(columns={'0': 'translated_titles_synopses'})\n",
    "  print(\"\\n Final Translated text: \")\n",
    "  print(translated_titles_synopses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_synopses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Vấn đề **`tiền xử lý ngôn ngữ tự nhiên`**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vì các data được gather / crawl từ các trang web => sẽ gặp một số **`lỗi định dạng khi get text từ các thẻ html`** -> cần processing\n",
    "\n",
    "Ngoài ra, sẽ áp dụng **`quy trình xử lý ngôn ngữ tự nhiên cơ bản`** nhằm mục đích giữ lại text mang nhiều ý nghĩa nhất có thể:\n",
    "  - Khoảng trắng ( str.strip )\n",
    "\n",
    "  - Xử lý viết tắt trong tiếng Anh (**`contractions`**)\n",
    "\n",
    "  - Xóa các kí tự dấu (X**`punctuation`**)\n",
    "  \n",
    "  - Xóa các từ phổ biến trong tiếng Anh (**`stopwords`**)\n",
    "\n",
    "  - Khôi phục về từ gốc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stop_words = stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "wl = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Remove html tags. Get a string as input and return the string without html tags.\n",
    "def remove_htmltags(html):\n",
    "    return BeautifulSoup(html).get_text()\n",
    "\n",
    "def remove_spacebar(text):\n",
    "    return text.strip()\n",
    "\n",
    "# Expanding contractions\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in punctuation])\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    text = \" \".join([word for word in tokens if word not in stop_words])\n",
    "    return text \n",
    "\n",
    "# def stemming(text):\n",
    "#     tokens = word_tokenize(text)\n",
    "#     stemmed_text=\" \".join([wl.lemmatize(word) for word in tokens])\n",
    "#     return stemmed_text\n",
    "\n",
    "def lemmatizing(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_text=\" \".join([wl.lemmatize(word) for word in tokens])\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprossed_translated_titles_synopses = translated_titles_synopses_df['translated_titles_synopses'].copy()\n",
    "\n",
    "# Applying text preprocessing\n",
    "preprossed_translated_titles_synopses = preprossed_translated_titles_synopses.map(lambda l: remove_htmltags(l))\n",
    "preprossed_translated_titles_synopses = preprossed_translated_titles_synopses.map(lambda l: remove_spacebar(l))\n",
    "preprossed_translated_titles_synopses = preprossed_translated_titles_synopses.map(lambda l: remove_punct(l))\n",
    "preprossed_translated_titles_synopses = preprossed_translated_titles_synopses.map(lambda l: expand_contractions(l))\n",
    "preprossed_translated_titles_synopses = preprossed_translated_titles_synopses.map(lambda l: remove_stopwords(l))\n",
    "#preprossed_translated_titles_synopses = preprossed_translated_titles_synopses.map(lambda l: stemming(l))\n",
    "preprossed_translated_titles_synopses = preprossed_translated_titles_synopses.map(lambda l: lemmatizing(l))\n",
    "\n",
    "preprossed_translated_titles_synopses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **`Topic Modelling`** with **`LDA`** (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(use_idf=True, norm='l2',ngram_range=(1, 1), max_df=0.9, min_df=0.001)\n",
    "tfidf_text = tfidf_vec.fit_transform(preprossed_translated_titles_synopses)\n",
    "print('TF-IDF output shape:', tfidf_text.shape)\n",
    "\n",
    "# n_components is the number of topics\n",
    "lda_model = LatentDirichletAllocation(n_components=25, random_state=random_state)\n",
    "lda_top = lda_model.fit_transform(tfidf_text)\n",
    "print(lda_top.shape) \n",
    "print('LDA output shape:', lda_top.shape) # (no_of_doc,no_of_topics)\n",
    "print(\"Final perplexity score on document set: \", lda_model.bound_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Xem thử tỉ lệ các topic mà cuốn sách đầu tiên`** có thể thuộc về"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composition of doc 0\n",
    "print(\"Document 0: \")\n",
    "for i,topic in enumerate(lda_top[0]):\n",
    "  print(\"Topic \",i,\": \",topic*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most likely topic for all docs\n",
    "for i, doc in enumerate(lda_top):\n",
    "    print(\"Document:\", i)\n",
    "    print(\"Most likely topic: \", np.argmax(doc), \": \",max(doc)*100,\"%\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important words for each topic\n",
    "vocab = tfidf_vec.get_feature_names_out()\n",
    "\n",
    "for i, comp in enumerate(lda_model.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_words:\n",
    "        print(t[0],end=\" \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lưu các topics** thành DataFrame mới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame(lda_top, columns=[\"Topic \" + i.__str__() for i in range(lda_top.shape[1])])\n",
    "topics_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cột **`Edition`**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Giải thích"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Edition`** đang có xu hướng gộp dữ liệu, ta có thể preprocessing để tách dữ liệu và tạo feature mới => Phục vụ việc **`Visualization`**\n",
    "\n",
    "**`Ví dụ: `**\n",
    "- **`Paperback,– 10 Mar 2016`** thì: **'Paperback'** là tòa soạn, **'10 Mar 2016'** là ngày tháng và năm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Một số sample có thêm kiểu phiên bản như: Import, Illustrated, Special Edition, Student Edition                                \n",
    "- **`Paperback,– Special Edition, 6 May 2008`** thì: **'Special Edition'** là kiểu phiên bản"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[data_df['Edition'].str.contains(\"Special Edition\")].head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngoài ra, ở một số ít sample **'Ngày'**, **'Tháng'**, **'Năm'** có thể bị miss => cần handle và xử lý.\n",
    "\n",
    "Ta sẽ cần split ',' chuỗi để lấy từng thành phần\n",
    "\n",
    "Khi split ',' sẽ chưa chính xác vì có 5 sample chứa quốc gia xuất bản đứng đầu như: (German), (Spanish) => Vì số lượng quá nhỏ nên ta loại bỏ luôn để đồng nhất dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the language string-text based on comma in two new columns df\n",
    "split_edition_df = data_df[\"Edition\"].str.split(\",\", n = 1, expand = True)\n",
    "\n",
    "# Marking the rows which contain the language property-tag in them\n",
    "language_list = []\n",
    "for item in split_edition_df[0]:\n",
    "    if '(' in item and ')' in item:\n",
    "        print(item)\n",
    "        language_list.append(item)\n",
    "    else:\n",
    "        language_list.append('NA')\n",
    "# Saving the marked rows in a new Pandas Series object        \n",
    "language_series = pd.Series(language_list)\n",
    "\n",
    "edition_with_removed_lang_series = data_df[\"Edition\"].copy()\n",
    "for i, element in enumerate(language_series):\n",
    "  if element != 'NA':\n",
    "    edition_with_removed_lang_series[i] = edition_with_removed_lang_series[i].replace(element+\",\", \"\", 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tách **`Tòa soạn`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_edition_df = edition_with_removed_lang_series.str.split(\",\", n = 1, expand = True)\n",
    "\n",
    "print_series = split_edition_df[0]\n",
    "print_series.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tách **`Năm Xuất Bản`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Define the function to remove the punctuations from the rows\n",
    "def remove_punctuations(text):\n",
    "    punctuations_to_remove = [punctuation for punctuation in string.punctuation if punctuation != ',']\n",
    "    punctuations_to_remove.append('–')\n",
    "    for punctuation in ['–']:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "rest_edition_series = split_edition_df[1].apply(remove_punctuations)\n",
    "\n",
    "print(\"Trước khi xóa\")\n",
    "print(split_edition_df[1][0:3])\n",
    "print(\"\")\n",
    "print(\"Sau khi xóa\")\n",
    "print(rest_edition_series[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year(text):\n",
    "    text = text[-4:]\n",
    "    return text\n",
    "\n",
    "year_series = rest_edition_series.apply(extract_year)\n",
    "year_series.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như ta thấy vẫn xuất hiện, những giá trị thuộc những sample sai cấu trúc => cho ra kết quả không phải **Năm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tổng các sample không phải là năm: ', (len(year_series) - sum(year_series.str.isnumeric())))\n",
    "year_series.loc[year_series.str.isnumeric() == False] = 'NA'\n",
    "print(\"Sau khi process: \", year_series.unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tách **`Tháng Xuất Bản`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = []\n",
    "\n",
    "for i, row in enumerate(rest_edition_series):\n",
    "  if year_series[i] != 'NA':\n",
    "    temp_data.append(row[:-5])\n",
    "  else:\n",
    "    temp_data.append(row)\n",
    "\n",
    "rest_edition_series = pd.Series(temp_data)\n",
    "\n",
    "def extract_month(text):\n",
    "    text = text[-3:]\n",
    "    return text\n",
    "\n",
    "month_series = rest_edition_series.apply(extract_month)\n",
    "month_series\n",
    "month_series.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngoài 12 tháng ra, vẫn còn xuất hiện lỗi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['Apr','Aug','Dec','Feb', 'Jan', 'Jul','Jun','Mar','May','Nov','Oct','Sep']\n",
    "for value in month_series:\n",
    "  if value not in months:\n",
    "    month_series = month_series.replace([value],'NA')\n",
    "\n",
    "print(month_series.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tách **`Kiểu Phiên Bản`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = []\n",
    "\n",
    "# Isolation of the rest of the text\n",
    "for i, row in enumerate(rest_edition_series):\n",
    "  if month_series[i] != 'NA':\n",
    "    temp_data.append(row[:-4])\n",
    "  else:\n",
    "    temp_data.append(row)\n",
    "\n",
    "rest_edition_series = pd.Series(temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the rest of the text by the comma\n",
    "rest_edition_list = [i.split(\",\") for i in list(rest_edition_series)]\n",
    "\n",
    "\n",
    "no_day_lists = []\n",
    "# Adding all the items of the rest of the text except the day of the month which was not included\n",
    "for a_list in rest_edition_list:\n",
    "    no_day_list = []\n",
    "    for item in a_list:\n",
    "        try:\n",
    "            int(item)\n",
    "        except ValueError:\n",
    "            if item != '':\n",
    "                no_day_list.append(item.strip())\n",
    "    no_day_lists.append(no_day_list)    \n",
    "    \n",
    "# Taking the resultunt text and casting it to a Series object    \n",
    "type_series = pd.Series(no_day_lists)\n",
    "# Marking as 'NA_kind' the rows/books which do not include any kind of text as a type \n",
    "type_series = type_series.apply(lambda y: 'NA_kind' if (len(y)==0) or (y == [''])   \n",
    "                                    else ','.join([elem.strip() for elem in y]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kết thúc Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cập nhật datase ban đầu thành data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprossed_data_df = data_df.copy()\n",
    "preprossed_data_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprossed_data_df = preprossed_data_df.drop(['Title', 'Synopsis', 'Author', 'Edition'], axis=1)\n",
    "#preprossed_data_df['Titles_synopses_translated'] = translated_titles_synopses_df['translated_titles_synopses'] # TODO PUT CLUSTERS INSTEAD\n",
    "preprossed_data_df['Reviews'] = reviews\n",
    "preprossed_data_df['Authors'] = authors\n",
    "preprossed_data_df['No. Authors'] = num_of_authors\n",
    "preprossed_data_df['Ratings'] = ratings\n",
    "\n",
    "# Add features extracted from the edition column\n",
    "preprossed_data_df['Print'] = print_series\n",
    "preprossed_data_df['Type'] = type_series\n",
    "preprossed_data_df['Month'] = month_series\n",
    "preprossed_data_df['Year'] = year_series\n",
    "\n",
    "# Add topic features\n",
    "preprossed_data_df = pd.concat([preprossed_data_df, topics_df], axis=1)\n",
    "\n",
    "preprossed_data_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data fill missing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill **`Năm bị thiếu`** (Dùng trung vị)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprossed_data_df['Year'] = preprossed_data_df['Year'].replace('NA', np.NaN)\n",
    "preprossed_data_df['Year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trước khi fill: \", preprossed_data_df.Year.isna().sum())\n",
    "preprossed_data_df['Year'] = preprossed_data_df['Year'].fillna(preprossed_data_df['Year'].median())\n",
    "print(\"Trước khi fill: \", preprossed_data_df.Year.isna().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill **`Tháng bị thiếu`**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random có trọng số"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprossed_data_df['Month'] = preprossed_data_df['Month'].replace('NA', np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprossed_data_df.Month.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprossed_data_df.Month.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probbabilities for each month\n",
    "probs = preprossed_data_df.Month.value_counts(normalize=True)\n",
    "\n",
    "preprossed_data_df.loc[preprossed_data_df.Month.isna(), 'Month'] = np.random.choice(probs.index, p=probs.values, \n",
    "                                                                   size=preprossed_data_df.Month.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprossed_data_df.Month.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprossed_data_df.Month.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclical Encoding for Month"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, month is a temporal feature which is categorized and cyclical feature, a specific encoding was implemented.  Cyclical features such as days or months of the year are treated as cyclical in the sense that their values display a cyclical pattern and are encoded as polar coordinates.\n",
    "\n",
    "With the polar representation, it was possible to assign different values for every moment in time while also hold on to the cyclical similarities and differences. After the months are enoded to numbers (1 to 12), the formulas for the cyclical-polar encoding, which two new features will arise, are the following:\n",
    "\n",
    "$$ month_{\\cos }=\\cos \\left(\\frac{2 \\pi \\times month}{\\max (month)}\\right)$$\n",
    "\n",
    "$$ month_{\\sin }=\\sin \\left(\\frac{2 \\pi \\times  month}{\\max (month)}\\right) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_to_int(df):\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr','May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    df['Month_Int'] = df['Month']\n",
    "    \n",
    "    for i, month in enumerate(months):\n",
    "        df['Month_Int'] = df['Month_Int'].replace(month, i+1)\n",
    "        \n",
    "    return pd.to_numeric(df[\"Month_Int\"], downcast=\"float\")\n",
    "    \n",
    "preprossed_data_df['Month_Int'] = month_to_int(preprossed_data_df)\n",
    "\n",
    "# Normalize x values to match with the 0-2π cycle\n",
    "preprossed_data_df[\"Month_Norm\"] = 2 * np.pi * preprossed_data_df[\"Month_Int\"] / preprossed_data_df[\"Month_Int\"].max()\n",
    "# Cos and sin features\n",
    "preprossed_data_df[\"Cos_Month\"] = np.cos(preprossed_data_df[\"Month_Norm\"])\n",
    "preprossed_data_df[\"Sin_Month\"] = np.sin(preprossed_data_df[\"Month_Norm\"])\n",
    "# preprossed_data_df[\"CONFIRM\"] = preprossed_data_df[\"Cos_Month\"]**2 + preprossed_data_df[\"Sin_Month\"]**2 # SHOULD BE ONE\n",
    "\n",
    "# Plotting months in a cyclical enocding\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=preprossed_data_df.Cos_Month, x=preprossed_data_df.Sin_Month, mode=\"markers\"))\n",
    "\n",
    "fig.update_layout(yaxis = dict(title=\"Cos_Month\"),\n",
    "                  xaxis = dict(title=\"Sin_Month\", scaleanchor = \"x\", scaleratio = 1))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting data for visualization and modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(preprossed_data_df.info())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Lưu dữ liệu đã preprocessing để **`visualization`** và **`xây dựng model`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move target variable to the end\n",
    "preprossed_data_df = preprossed_data_df[[c for c in preprossed_data_df if c not in ['Price']] \n",
    "                  + ['Price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data_df =  preprossed_data_df.drop(['Month', 'Month_Int', 'Month_Norm'], axis=1)\n",
    "visualization_data_df =  preprossed_data_df.drop(['Month_Int', 'Month_Norm'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving-exporting to new .csv file\n",
    "finalized_data_df.to_csv('finalized_data_df.csv', index=False)\n",
    "visualization_data_df.to_csv('visualization_data_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
